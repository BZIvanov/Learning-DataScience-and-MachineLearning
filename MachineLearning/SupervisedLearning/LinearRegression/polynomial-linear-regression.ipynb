{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e158548",
   "metadata": {},
   "source": [
    "# Polynomial Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6602c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168aeb9",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17058e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = np.array([181,9,58,120,9,200,66,215,24,98,204,195,68,281,69,147,218,237,13,228,62,263,143,240,249])\n",
    "radio = np.array([11,49,33,20,2,3,6,24,35,8,33,48,37,40,21,24,28,5,16,17,13,4,29,17,27])\n",
    "newspaper = np.array([58,75,24,12,1,21,24,4,66,7,46,53,114,56,18,19,53,24,50,26,18,20,13,23,23])\n",
    "sales = np.array([13,7,12,13,5,11,9,17,9,10,19,22,13,24,11,15,18,13,6,16,10,12,15,16,19])\n",
    "\n",
    "df = pd.DataFrame({'tv': tv, 'radio': radio, 'newspaper': newspaper, 'sales': sales})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70fa829",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sales', axis=1) # keep only the features\n",
    "y = df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_converter = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter \"fits\" to data, in this case, reads in every X column\n",
    "# Then it \"transforms\" and ouputs the new polynomial data\n",
    "poly_features = polynomial_converter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acec12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547fcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 3 items are our original items from X\n",
    "# the 4th item is square of feature1 (181 * 181)\n",
    "# the 5th item feature1 * feature2 (181 * 11)\n",
    "# the 6th item feature1 * feature3 (181 * 58)\n",
    "# the 7th item is square of feature2 (11 * 11)\n",
    "# the 8th item feature2 * feature3 (11 * 58)\n",
    "# the 9th item is square of feature3 (58 * 58)\n",
    "poly_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd881c49",
   "metadata": {},
   "source": [
    "Now after on base of our 3 initial features we have 9 features and we will use this new polynomial regression set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345bf7b",
   "metadata": {},
   "source": [
    "## Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adea8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now predicting y (sales) based on our new dataset with 9 features\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3691fa3",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ee0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da795b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4effd1",
   "metadata": {},
   "source": [
    "## Predictions on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97209a52",
   "metadata": {},
   "source": [
    "We want to fairly evaluate our model, so we get performance metrics on the test set (data the model has never seen before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be968d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608fc89",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAE)\n",
    "print(MSE)\n",
    "print(RMSE)\n",
    "print(df['sales'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67808d",
   "metadata": {},
   "source": [
    "### Comparison with Simple Linear Regression\n",
    "\n",
    "Results on the Test Set (Note: Use the same Random Split to fairly compare)\n",
    "\n",
    "- Simple Linear Regression:\n",
    "  - MAE: 2.0326719009660215\n",
    "  - RMSE: 2.391906607310042\n",
    "\n",
    "- Polynomial 2-degree:\n",
    "  - MAE: 0.9084497157236153\n",
    "  - RMSE: 1.1193077657280737\n",
    "  \n",
    "We can see that the polynomial regression is performing better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f06cd",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "- model will perform very well on training data, but will have poor performance on new unseen data\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "- model does not capture the underlying trend of the data and does not fit the data well enough\n",
    "- can lead to poor performance in both training and testing data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e1085",
   "metadata": {},
   "source": [
    "## Choosing a Model\n",
    "\n",
    "### Adjusting Parameters\n",
    "\n",
    "Are we satisfied with this performance? Perhaps a higher order would improve performance even more! But how high is too high? It is now up to us to possibly go back and adjust our model and parameters, let's explore higher order Polynomials in a loop and plot out their error. This will nicely lead us into a discussion on Overfitting.\n",
    "\n",
    "Let's use a for loop to do the following:\n",
    "\n",
    "1. Create different order polynomial X data\n",
    "2. Split that polynomial data for train/test\n",
    "3. Fit on the training data\n",
    "4. Report back the metrics on *both* the train and test results\n",
    "5. Plot these results and explore overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19926564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ERROR PER DEGREE\n",
    "train_rmse_errors = []\n",
    "# TEST ERROR PER DEGREE\n",
    "test_rmse_errors = []\n",
    "\n",
    "for d in range(1, 10):\n",
    "    # CREATE POLY DATA SET FOR DEGREE \"d\"\n",
    "    polynomial_converter = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    poly_features = polynomial_converter.fit_transform(X)\n",
    "    \n",
    "    # SPLIT THIS NEW POLY DATA SET\n",
    "    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=99)\n",
    "    \n",
    "    # TRAIN ON THIS NEW POLY SET\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # PREDICT ON BOTH TRAIN AND TEST\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Errors\n",
    "    \n",
    "    # Errors on Train Set\n",
    "    train_RMSE = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    \n",
    "    # Errors on Test Set\n",
    "    test_RMSE = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "    # Append errors to lists for plotting later\n",
    "    \n",
    "   \n",
    "    train_rmse_errors.append(train_RMSE)\n",
    "    test_rmse_errors.append(test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124172b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd720f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 10), train_rmse_errors, label='TRAIN RMSE')\n",
    "plt.plot(range(1, 10), test_rmse_errors, label='TEST RMSE')\n",
    "plt.xlabel(\"Polynomial Complexity\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979db0c2",
   "metadata": {},
   "source": [
    "Looking at the plot below it looks like the best `degree` value for us is 2, after that things are getting really far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above, but in more focused on the closer part\n",
    "plt.plot(range(1, 4), train_rmse_errors[:3], label='TRAIN RMSE')\n",
    "plt.plot(range(1, 4), test_rmse_errors[:3], label='TEST RMSE')\n",
    "plt.xlabel(\"Polynomial Complexity\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5cf5c9",
   "metadata": {},
   "source": [
    "## Finalizing Model Choice\n",
    "\n",
    "There are now 2 things we need to save, the Polynomial Feature creator AND the model itself. Let's explore how we would proceed from here:\n",
    "\n",
    "1. Choose final parameters based on test metrics\n",
    "2. Retrain on all data\n",
    "3. Save Polynomial Converter object\n",
    "4. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our chart, we select degree=2\n",
    "final_poly_converter = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4349c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b16414",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(final_poly_converter.fit_transform(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9f665",
   "metadata": {},
   "source": [
    "### Saving Model and Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864edb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(final_model, 'sales_poly_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29563fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(final_poly_converter,'poly_converter.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfcf26f",
   "metadata": {},
   "source": [
    "## Deployment and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b1e1f",
   "metadata": {},
   "source": [
    "### Prediction on New Data\n",
    "\n",
    "Recall that we will need to **convert** any incoming data to polynomial data, since that is what our model is trained on. We simply load up our saved converter object and only call **.transform()** on the new data, since we're not refitting to a new data set.\n",
    "\n",
    "**Our next ad campaign will have a total spend of 149 on TV, 22 on Radio, and 12 on Newspaper Ads, how many units could we expect to sell as a result of this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e72562",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_poly = load('poly_converter.joblib')\n",
    "loaded_model = load('sales_poly_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = np.array([149])\n",
    "cradio = np.array([22])\n",
    "cnewspaper = np.array([12])\n",
    "\n",
    "campaign = pd.DataFrame({'tv': ctv, 'radio': cradio, 'newspaper': cnewspaper})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b410e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_poly = loaded_poly.fit_transform(campaign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8671d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.predict(campaign_poly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
